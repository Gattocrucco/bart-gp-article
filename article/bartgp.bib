@book{dlmf,
	date-modified = {2022-07-28 17:00:23 +0200},
	howpublished = {Release 1.1.6 of 2022-06-30},
    year = {2022},
	author = {F.~W.~J. Olver and A.~B. {Olde Daalhuis} and D.~W. Lozier and B.~I. Schneider and R.~F. Boisvert and C.~W. Clark and B.~R. Miller and B.~V. Saunders and H.~S. Cohl and M.~A. McClain and others},
	title = {NIST Digital Library of Mathematical Functions},
	url = {http://dlmf.nist.gov/},
	bdsk-url-1 = {http://dlmf.nist.gov/},
}

@book{rasmussen2006,
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	date-added = {2021-07-14 17:27:09 +0200},
	date-modified = {2021-07-15 12:57:02 +0200},
	isbn = {0-262-18253-X},
	title = {Gaussian Processes for Machine Learning},
	url = {http://www.gaussianprocess.org/gpml/},
	year = 2006,
	Bdsk-Url-1 = {http://www.gaussianprocess.org/gpml/},
}

@book{stein1999,
    year = {1999},
    title = {Interpolation of Spatial Data},
    subtitle = {Some Theory for Kriging},
    author = {Stein, Michael L.},
    doi = {10.1007/978-1-4612-1494-6},
    series = {Springer Series in Statistics},
    publisher = {Springer New York, NY},
    issn = {0172-7397, 2197-568X},
    isbn = {978-0-387-98629-6, 978-1-4612-7166-6, 978-1-4612-1494-6},
}

@book{gramacy2020,
  author    = {Gramacy, Robert B.},
  title     = {Surrogates: {Gaussian} Process Modeling, Design, and Optimization for the Applied Sciences},
  year      = {2020},
  publisher = {Chapman and Hall/CRC},
  address   = {New York},
  edition   = {1},
  month     = {1},
  doi       = {10.1201/9780367815493},
  isbn      = {9780367815493},
  pages     = {560},
  url       = {https://doi.org/10.1201/9780367815493}
}

@book{wendland2004,
    place={Cambridge},
    series={Cambridge Monographs on Applied and Computational Mathematics},
    title={Scattered Data Approximation},
    DOI={10.1017/CBO9780511617539},
    publisher={Cambridge University Press},
    author={Wendland, Holger},
    year={2004},
    collection={Cambridge Monographs on Applied and Computational Mathematics},
    isbn = {978-0-511-61753-9, 978-0-521-13101-8}}

@book{gradshtein2014,
    title = {Table of Integrals, Series, and Products},
    author = {Gradshteyn, I. S. and Ryzhik, I. M.},
    editor = {Zwillinger, Daniel and Moll, Victor},
    url = {http://www.mathtable.com/gr},
    publisher = {Academic Press, Elsevier},
    isbn = {978-0-12-384933-5},
    year = {2014},
    edition = {8},
}

@inproceedings{balog2016,
  author = {Matej Balog and Balaji Lakshminarayanan and Zoubin Ghahramani and Daniel M.~Roy and Yee Whye Teh},
  title={The {M}ondrian Kernel},
  booktitle = {32nd Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = {2016},
  month = {June},
  url = {https://www.auai.org/uai2016/proceedings/papers/236.pdf}
}

@article{chipman2010,
    author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
    title = {{BART: Bayesian additive regression trees}},
    volume = {4},
    journal = {The Annals of Applied Statistics},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {266 -- 298},
    keywords = {Bayesian backfitting, boosting, CART, classification, ensemble, MCMC, Nonparametric regression, probit model, random basis, regularizatio, sum-of-trees model, Variable selection, weak learner},
    year = {2010},
    doi = {10.1214/09-AOAS285},
    addendum = {Introduces BART.}
}

@inproceedings{he2019,
    title = 	 {XBART: Accelerated Bayesian Additive Regression Trees},
    author =       {He, Jingyu and Yalov, Saar and Hahn, P. Richard},
    booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
    pages = 	 {1130--1138},
    year = 	 {2019},
    editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
    volume = 	 {89},
    series = 	 {Proceedings of Machine Learning Research},
    publisher =    {PMLR},
    url = 	 {https://proceedings.mlr.press/v89/he19a.html},
    abstract = 	 {Bayesian additive regression trees (BART) (Chipman et. al., 2010) is a powerful predictive model that often outperforms alternative models at out-of-sample prediction. BART is especially well-suited to settings with unstructured predictor variables and substantial sources of unmeasured variation as is typical in the social, behavioral and health sciences. This paper develops a modified version of BART that is amenable to fast posterior estimation. We present a stochastic hill climbing algorithm that matches the remarkable predictive accuracy of previous BART implementations, but is many times faster and less memory intensive.  Simulation studies show that the new method is comparable in computation time and more accurate at function estimation than both random forests and gradient boosting.},
    addendum = {Faster algorithm for BART inference.}
}

@article{hill2011,
    author = {Jennifer L. Hill},
    title = {Bayesian Nonparametric Modeling for Causal Inference},
    journal = {Journal of Computational and Graphical Statistics},
    volume = {20},
    number = {1},
    pages = {217-240},
    year  = {2011},
    publisher = {Taylor & Francis},
    doi = {10.1198/jcgs.2010.08162},
    addendum = {Introduces the use of BART in causal inference.}
}


@InProceedings{rockova2019,
    title = 	 {On Theory for BART},
    author =       {Ro\v{c}kov\'a, Veronika and Saha, Enakshi},
    booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
    pages = 	 {2839--2848},
    year = 	 {2019},
    editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
    volume = 	 {89},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {4},
    publisher =    {PMLR},
    url = 	 {https://proceedings.mlr.press/v89/rockova19a.html},
    abstract = 	 {Ensemble learning is a statistical paradigm built on the premise that  many weak learners can perform exceptionally well when deployed collectively. The BART method of Chipman et al. (2010) is a prominent example of Bayesian ensemble learning, where each learner is a tree. Due to its impressive performance, BART has received a lot of attention from practitioners. Despite its wide popularity, however, theoretical studies of BART have  begun emerging only very recently. Laying down foundation for the theoretical analysis of Bayesian forests,  Rockova and van der Pas (2017) showed optimal posterior concentration under conditionally uniform tree priors. These priors  deviate from the actual priors implemented in BART. Here, we study the exact BART prior and propose a simple modification so that  it also enjoys optimality properties. To this end, we dive into the branching processes theory. We obtain  tail bounds for the distribution of total progeny under heterogeneous Galton-Watson (GW) processes using their connection to random walks. We conclude with a result stating  optimal rate of convergence for BART.},
    addendum = {Posterior concetration rates.}
}

@article{hill2020,
   author = "Hill, Jennifer L. and Linero, Antonio R. and Murray, Jared",
   title = "Bayesian Additive Regression Trees: A Review and Look Forward", 
   journal= "Annual Review of Statistics and Its Application",
   year = "2020",
   volume = "7",
   number = "Volume 7, 2020",
   pages = "251-278",
   doi = "https://doi.org/10.1146/annurev-statistics-031219-041110",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041110",
   publisher = "Annual Reviews",
   issn = "2326-831X",
   type = "Journal Article",
   keywords = "Bayesian nonparametrics",
   keywords = "regression",
   keywords = "regularization",
   keywords = "machine learning",
   keywords = "causal inference",
   abstract = "Bayesian additive regression trees (BART) provides a flexible approach to fitting a variety of regression models while avoiding strong parametric assumptions. The sum-of-trees model is embedded in a Bayesian inferential framework to support uncertainty quantification and provide a principled approach to regularization through prior specification. This article presents the basic approach and discusses further development of the original algorithm that supports a variety of data structures and assumptions. We describe augmentations of the prior specification to accommodate higher dimensional data and smoother functions. Recent theoretical developments provide justifications for the performance observed in simulations and other settings. Use of BART in causal inference provides an additional avenue for extensions and applications. We discuss software options as well as challenges and future directions.",
  }

@article{linero2018,
    author = {Linero, Antonio R.},
    title = {Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection},
    journal = {Journal of the American Statistical Association},
    volume = {113},
    number = {522},
    pages = {626-636},
    year  = {2018},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.2016.1264957},
    addendum = {A modification of BART which is
            much better at variable selection.}
}

@misc{linero2022,
    doi = {10.48550/ARXIV.2210.16375},
    author = {Linero, Antonio R.},
    keywords = {Methodology (stat.ME), Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {SoftBart: Soft Bayesian Additive Regression Trees},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license},
    addendum = {The
          most recent, state of the art package for BART. Features priors on the
          hyperparameters and provides an interface to raw gibbs steps to use
          BART within other models (although not arbitrary ones).}
}

@misc{maia2022,
    doi = {10.48550/ARXIV.2204.02112},
    author = {Maia, Mateus and Murphy, Keefe and Parnell, Andrew C.},
    keywords = {Methodology (stat.ME), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International},
    addendum = {GP in the BART leaves.}
}

@misc{quiroga2022,
    doi = {10.48550/ARXIV.2206.03619},
    author = {Quiroga, Miriana and Garay, Pablo G and Alonso, Juan M. and Loyola, Juan Martin and Martin, Osvaldo A},
    keywords = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Bayesian additive regression trees for probabilistic programming},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution Share Alike 4.0 International},
    addendum = {An useful adaptation to use BART within PyMC. Note that
          they keep the hyperparameters fixed, and do not support more than one
          BART in the model.}
}

@misc{ronen2022,
    doi = {10.48550/ARXIV.2210.09352},
    author = {Ronen, Omer and Saarinen, Theo and Tan, Yan Shuo and Duncan, James and Yu, Bin},
    keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
    title = {A Mixing Time Lower Bound for a Simplified Version of BART},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license},
    addendum = {Says that BART's mcmc is slow to mix, with mixing time scaling
        badly with dataset size.}
}

@article{linero2017,
    doi = {10.29220/csam.2017.24.6.543},
    author = {Linero, Antonio R.},
    title = {A review of tree-based Bayesian methods},
    journal = {Communications for Statistical Applications and Methods},
    year = {2017},
    pages = {543–-559},
    volume = {24},
    number = {6},
}

@article{sparapani2021,
    title={Nonparametric Machine Learning and Efficient Computation with Bayesian Additive Regression Trees: The BART R Package},
    volume={97},
    doi={10.18637/jss.v097.i01},
    abstract={In this article, we introduce the BART R package which is an acronym for Bayesian additive regression trees. BART is a Bayesian nonparametric, machine learning, ensemble predictive modeling method for continuous, binary, categorical and time-to-event outcomes. Furthermore, BART is a tree-based, black-box method which fits the outcome to an arbitrary random function, f , of the covariates. The BART technique is relatively computationally efficient as compared to its competitors, but large sample sizes can be demanding. Therefore, the BART package includes efficient state-of-the-art implementations for continuous, binary, categorical and time-to-event outcomes that can take advantage of modern off-the-shelf hardware and software multi-threading technology. The BART package is written in C++ for both programmer and execution efficiency. The BART package takes advantage of multi-threading via forking as provided by the parallel package and OpenMP when available and supported by the platform. The ensemble of binary trees produced by a BART fit can be stored and re-used later via the R predict function. In addition to being an R package, the installed BART routines can be called directly from C++. The BART package provides the tools for your BART toolbox.},
    number={1},
    journal={Journal of Statistical Software},
    author={Sparapani, Rodney and Spanbauer, Charles and McCulloch, Robert},
    year={2021},
    pages={1–66}
}        

@article{hahn2020,
    author = {P. Richard Hahn and Jared S. Murray and Carlos M. Carvalho},
    title = {{Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion)}},
    volume = {15},
    journal = {Bayesian Analysis},
    number = {3},
    publisher = {International Society for Bayesian Analysis},
    pages = {965 -- 2020},
    keywords = {Bayesian, Causal inference, heterogeneous treatment effects, machine learning, predictor-dependent priors, regression trees, regularization, shrinkage},
    year = {2020},
    doi = {10.1214/19-BA1195},
    addendum = {BCF.}
}

@misc{wang2022,
    doi = {10.48550/ARXIV.2204.10963},
    author = {Wang, Meijiang and He, Jingyu and Hahn, P. Richard},
    keywords = {Methodology (stat.ME), Econometrics (econ.EM), Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Economics and business, FOS: Economics and business},
    title = {Local Gaussian process extrapolation for BART models with applications to causal inference},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International},
    addendum = {Graft a Gaussian process outside the grid to extrapolate better.}
}

@misc{li2022,
    doi = {10.48550/ARXIV.2206.15460},
    author = {Li, Fan and Ding, Peng and Mealli, Fabrizia},
    keywords = {Methodology (stat.ME), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Bayesian Causal Inference: A Critical Review},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution 4.0 International},
}

@article{dorie2019,
    author = {Vincent Dorie and Jennifer Hill and Uri Shalit and Marc Scott and Dan Cervone},
    title = {{Automated versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition}},
    volume = {34},
    journal = {Statistical Science},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {43 -- 68},
    keywords = {automated algorithms, Causal inference, competition, evaluation, machine learning},
    year = {2019},
    doi = {10.1214/18-STS667},
}

@article{chipman1998,
    author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
    title = {Bayesian CART Model Search},
    journal = {Journal of the American Statistical Association},
    volume = {93},
    number = {443},
    pages = {935-948},
    year  = {1998},
    publisher = {Taylor & Francis},
    doi = {10.1080/01621459.1998.10473750},
    addendum = {The article which introduces the single-tree version of BART.}
}

@article{linero2018b,
    author = {Linero, Antonio R. and Yang, Yun},
    title = {Bayesian regression tree ensembles that adapt to smoothness and sparsity},
    journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    volume = {80},
    number = {5},
    pages = {1087-1110},
    keywords = {Bayesian additive regression trees, Bayesian non-parametrics, High dimensional regimes, Model averaging, Posterior consistency},
    doi = {https://doi.org/10.1111/rssb.12293},
    url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12293},
    eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12293},
    abstract = {Summary Ensembles of decision trees are a useful tool for obtaining flexible estimates of regression functions. Examples of these methods include gradient-boosted decision trees, random forests and Bayesian classification and regression trees. Two potential shortcomings of tree ensembles are their lack of smoothness and their vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework and illustrate its promising performance through testing on benchmark data sets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up to a logarithmic factor) for sparse functions and functions with additive structures in the high dimensional regime where the dimensionality of the covariate space is allowed to grow nearly exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modifications to existing Bayesian additive regression tree algorithms.},
    year = {2018}
}

@book{tong1990,
    doi = {10.1007/978-1-4613-9655-0},
    year = {1990},
    publisher = {Springer New York},
    author = {Tong, Y. L.},
    title = {The Multivariate Normal Distribution},
    isbn = {978-1-4613-9657-4},
    edition = {1},
    issn = {0172-7397},
}

@article{scipy,
    author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
    title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
    journal = {Nature Methods},
    year    = {2020},
    volume  = {17},
    pages   = {261--272},
    adsurl  = {https://rdcu.be/b08Wh},
    doi     = {10.1038/s41592-019-0686-2},
}

@article{harris2020,
    title         = {Array programming with {NumPy}},
    author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
    year          = {2020},
    month         = {sep},
    journal       = {Nature},
    volume        = {585},
    number        = {7825},
    pages         = {357--362},
    doi           = {10.1038/s41586-020-2649-2},
    publisher     = {Springer Science and Business Media {LLC}},
    url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{pymc,
    doi = {10.7717/peerj-cs.55},
    url = {https://doi.org/10.7717/peerj-cs.55},
    year  = {2016},
    month = {apr},
    publisher = {{PeerJ}},
    volume = {2},
    pages = {e55},
    author = {John Salvatier and Thomas V. Wiecki and Christopher Fonnesbeck},
    title = {Probabilistic programming in Python using {PyMC}3},
    journal = {{PeerJ} Computer Science}
}

@article{matplotlib,
    Author    = {Hunter, J. D.},
    Title     = {Matplotlib: A 2D graphics environment},
    Journal   = {Computing in Science \& Engineering},
    Volume    = {9},
    Number    = {3},
    Pages     = {90--95},
    abstract  = {Matplotlib is a 2D graphics package used for Python for
    application development, interactive scripting, and publication-quality
    image generation across user interfaces and operating systems.},
    publisher = {IEEE COMPUTER SOC},
    doi       = {10.1109/MCSE.2007.55},
    year      = 2007
}

@article{pratola2020,
author = {M. T. Pratola and H. A. Chipman and E. I. George and R. E. McCulloch},
title = {Heteroscedastic BART via Multiplicative Regression Trees},
journal = {Journal of Computational and Graphical Statistics},
volume = {29},
number = {2},
pages = {405-417},
year  = {2020},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2019.1677243},

URL = { 
    
        https://doi.org/10.1080/10618600.2019.1677243
    
    

},
eprint = { 
    
        https://doi.org/10.1080/10618600.2019.1677243
    
    

}

}

@article{kim2007,
author = { Hyunjoong   Kim  and  Wei-Yin   Loh  and  Yu-Shan   Shih  and  Probal   Chaudhuri },
title = {Visualizable and interpretable regression models with good prediction power},
journal = {IIE Transactions},
volume = {39},
number = {6},
pages = {565-579},
year  = {2007},
publisher = {Taylor & Francis},
doi = {10.1080/07408170600897502},

URL = { 
    
        https://doi.org/10.1080/07408170600897502
    
    

},
eprint = { 
    
        https://doi.org/10.1080/07408170600897502
    
    

}

}

@article{kapelner2016,
 title={bartMachine: Machine Learning with Bayesian Additive Regression Trees},
 volume={70},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v070i04},
 doi={10.18637/jss.v070.i04},
 abstract={We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data.},
 number={4},
 journal={Journal of Statistical Software},
 author={Kapelner, Adam and Bleich, Justin},
 year={2016},
 pages={1–40}
}

@article{jeong2023,
  author  = {Seonghyun Jeong and Veronika Rockova},
  title   = {The Art of BART: Minimax Optimality over Nonhomogeneous Smoothness in High Dimension},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {337},
  pages   = {1--65},
  url     = {http://jmlr.org/papers/v24/22-0382.html}
}

@book{imbens2015,
    place={Cambridge},
    title={Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction},
    DOI={10.1017/CBO9781139025751},
    publisher={Cambridge University Press},
    author={Imbens, Guido W. and Rubin, Donald B.},
    year={2015},
}

@book{gelman2013, 
    author={Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
    year={2013},
    title={Bayesian Data Analysis},
    edition={3},
    publisher={Chapman and Hall/CRC},
    doi={10.1201/b16018},
    isbn={9780429113079},
    location={New York},
}

@inproceedings{gardner2018,
 author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
 url = {https://proceedings.neurips.cc/paper/2018/hash/27e8e17134dd7083b050476733207ea1-Abstract.html},
 volume = {31},
 year = {2018}
}

@article{quinonero2005,
  author  = {Joaquin Qui{{\~n}}onero-Candela and Carl Edward Rasmussen},
  title   = {A Unifying View of Sparse Approximate Gaussian Process Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {65},
  pages   = {1939--1959},
  url     = {http://jmlr.org/papers/v6/quinonero-candela05a.html}
}

@article{tan2019,
author = {Tan, Yaoyuan Vincent and Roy, Jason},
title = {Bayesian additive regression trees and the General BART model},
journal = {Statistics in Medicine},
volume = {38},
number = {25},
pages = {5048-5069},
keywords = {Bayesian nonparametrics, Dirichlet process mixtures, machine learning, semiparametric models, spatial},
doi = {10.1002/sim.8347},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8347},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8347},
abstract = {Bayesian additive regression trees (BART) is a flexible prediction model/machine learning approach that has gained widespread popularity in recent years. As BART becomes more mainstream, there is an increased need for a paper that walks readers through the details of BART, from what it is to why it works. This tutorial is aimed at providing such a resource. In addition to explaining the different components of BART using simple examples, we also discuss a framework, the General BART model that unifies some of the recent BART extensions, including semiparametric models, correlated outcomes, and statistical matching problems in surveys, and models with weaker distributional assumptions. By showing how these models fit into a single framework, we hope to demonstrate a simple way of applying BART to research problems that go beyond the original independent continuous or binary outcomes framework.},
year = {2019}
}

@book{vandervaart1998,
    place={Cambridge},
    series={Cambridge Series in Statistical and Probabilistic Mathematics},
    title={Asymptotic Statistics},
    publisher={Cambridge University Press},
    author={{van der Vaart}, A. W.},
    year={1998},
    collection={Cambridge Series in Statistical and Probabilistic Mathematics},
    isbn = {9780511802256},
    doi={10.1017/CBO9780511802256}
}

@misc{imai2022,
  doi = {10.48550/ARXIV.2203.14511},
  url = {https://arxiv.org/abs/2203.14511},
  author = {Imai, Kosuke and Li, Michael Lingzhi},
  keywords = {Methodology (stat.ME), Applications (stat.AP), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Statistical Inference for Heterogeneous Treatment Effects Discovered by Generic Machine Learning in Randomized Experiments},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{imai2024,
    author = {Kosuke Imai and Michael Lingzhi Li},
    title = {Statistical Inference for Heterogeneous Treatment Effects Discovered by Generic Machine Learning in Randomized Experiments},
    journal = {Journal of Business \& Economic Statistics},
    volume = {0},
    number = {0},
    pages = {1--13},
    year = {2024},
    publisher = {ASA Website},
    doi = {10.1080/07350015.2024.2358909},
    URL = {https://doi.org/10.1080/07350015.2024.2358909},
    eprint = {https://doi.org/10.1080/07350015.2024.2358909}
}

@article{imai2013,
author = {Kosuke Imai and Marc Ratkovic},
title = {{Estimating treatment effect heterogeneity in randomized program evaluation}},
volume = {7},
journal = {The Annals of Applied Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {443 -- 470},
keywords = {Causal inference, Individualized treatment rules, Lasso, moderation, Variable selection},
year = {2013},
doi = {10.1214/12-AOAS593},
URL = {https://doi.org/10.1214/12-AOAS593}
}

@article{linero2022b,
author = {Linero, Antonio R. and Antonelli, Joseph L.},
title = {The how and why of Bayesian nonparametric causal inference},
journal = {WIREs Computational Statistics},
year = {2022},
pages = {e1583},
keywords = {decision trees, Gaussian processes, mediation analysis, mixture models, nonparametrics},
doi = {10.1002/wics.1583},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1583},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1583},
abstract = {Abstract Spurred on by recent successes in causal inference competitions, Bayesian nonparametric (and high-dimensional) methods have recently seen increased attention in the causal inference literature. In this article, we present a comprehensive overview of Bayesian nonparametric applications to causal inference. Our aims are to (i) introduce the fundamental Bayesian nonparametric toolkit; (ii) discuss how to determine which tool is most appropriate for a given problem; and (iii) show how to avoid common pitfalls in applying Bayesian nonparametric methods in high-dimensional settings. Unlike standard fixed-dimensional parametric problems, where outcome modeling alone can sometimes be effective, we argue that most of the time it is necessary to model both the selection and outcome processes. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Analysis of High Dimensional Data Statistical and Graphical Methods of Data Analysis > Nonparametric Methods Statistical and Graphical Methods of Data Analysis > Bayesian Methods and Theory}
}

@book{schott2017,
    author = {Schott, James R.},
    isbn = {9781119092483},
    title = {Matrix analysis for statistics},
    edition = {3},
    year = {2017},
    publisher = {John Wiley \& Sons},
    location = {Hoboken, New Jersey},
}

@software{dorie2024,
    author = {Dorie, Vincent and
          Chipman, Hugh and
          McCulloch, Robert and
          Dadgar, Armon and
          {R Core Team} and
          Draheim, Guido U. and
          Bosmans, Maarten and
          Tournayre, Christophe and
          Petch, Michael and
          Valle, Rafael de Lucena and
          Johnson, Steven G. and
          Frigo, Matteo and
          Zaitseff, John and
          Veldhuizen, Todd and
          Maisonobe, Luc and
          Pakin, Scott and
          Richard G., Daniel},
    title = {dbarts: Discrete Bayesian Additive Regression Trees Sampler},
    year = {2024},
    version = {0.9-28},
    url = {https://CRAN.R-project.org/package=dbarts},
}

@software{kapelner2023,
    author = {Kapelner, Adam and Bleich, Justin},
    title = {bartMachine: Bayesian Additive Regression Trees},
    year = {2023},
    version = {1.3.4.1},
    url = {https://cran.r-project.org/package=bartMachine},
}

@software{mcculloch2024,
    author = {McCulloch, Robert and Sparapani, Rodney and Gramacy, Robert and Pratola, Matthew  and Spanbauer, Charles  and Plummer, Martyn  and Best, Nicky  and Cowles, Kate  and Vines, Karen },
    title = {BART: Bayesian Additive Regression Trees},
    year = {2024},
    version = {2.9.9},
    url = {https://cran.r-project.org/package=BART},
}

@software{chipman2024,
    author = {Chipman, Hugh A. and McCulloch, Robert },
    title = {BayesTree: Bayesian Additive Regression Trees},
    year = {2024},
    version = {0.3-1.5},
    url = {https://cran.r-project.org/package=BayesTree},
}

@misc{dua2019,
    author = "Dua, Dheeru and Graff, Casey",
    year = "2017",
    title = "{UCI} Machine Learning Repository",
    url = "http://archive.ics.uci.edu/ml",
    institution = "University of California, Irvine, School of Information and Computer Sciences",
}

@article{thal2023,
    author = {Thal, Dan R.C. and Finucane, Mariel M.},
    title = {Causal Methods Madness: Lessons Learned from the 2022 ACIC Competition to Estimate Health Policy Impacts},
    journal = {Observational Studies},
    volume = {9},
    number = {3},
    year = {2023},
    pages = {3-27},
    doi = {10.1353/obs.2023.0023},
}

@online{lipman2022,
  author = {Lipman, Erin and Thal, Dan R.C. and Finucane, Mariel M.},
  title = {American Causal Inference Conference 2022 Data Challenge},
  year = {2022},
  url = {https://acic2022.mathematica.org/},
  urldate = {2024-10-10},
  organization = {Mathematica}
}

@misc{deshpande2023,
      title={flexBART: Flexible Bayesian regression trees with categorical predictors}, 
      author={Sameer K. Deshpande},
      year={2023},
      eprint={2211.04459},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{yeo2000,
    author = {Yeo, In‐Kwon and Johnson, Richard A.},
    title = "{A new family of power transformations to improve normality or symmetry}",
    journal = {Biometrika},
    volume = {87},
    number = {4},
    pages = {954-959},
    year = {2000},
    month = {12},
    abstract = "{We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box–Cox transformation for positive variables. The large‐sample properties of the transformation are investigated in the contect of a single random sample.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/87.4.954},
    url = {https://doi.org/10.1093/biomet/87.4.954},
    eprint = {https://academic.oup.com/biomet/article-pdf/87/4/954/633221/870954.pdf},
}

@book{murphy2023,
    author = "Kevin P. Murphy",
    title = "Probabilistic Machine Learning: Advanced Topics",
    publisher = "MIT Press",
    year = 2023,
    url = "http://probml.github.io/book2",
    date = {2023-08-14},
}

@article{rubin1974,
    author = {Rubin, D. B.}, 
    title = {Estimating causal effects of treatments in randomized and nonrandomized studies},
    journal = {Journal of Educational Psychology},
    volume = {66},
    number = {5},
    pages = {688--701},
    year = {1974},
    publisher = {American Psychological Association},
    doi = {10.1037/h0037350},
}

@book{pearl2009,
    place={Cambridge},
    edition={2},
    title={Causality},
    DOI={10.1017/CBO9780511803161},
    publisher={Cambridge University Press},
    author={Pearl, Judea},
    year={2009},
    isbn={9780511803161},
}

@article{horii2023,
    title={Uncertainty Quantification in Heterogeneous Treatment Effect Estimation with Gaussian-Process-Based Partially Linear Model},
    volume={38},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/30025},
    DOI={10.1609/aaai.v38i18.30025},
    abstractNote={Estimating heterogeneous treatment effects across individuals has attracted growing attention as a statistical tool for performing critical decision-making. We propose a Bayesian inference framework that quantifies the uncertainty in treatment effect estimation to support decision-making in a relatively small sample size setting. Our proposed model places Gaussian process priors on the nonparametric components of a semiparametric model called a partially linear model. This model formulation has three advantages. First, we can analytically compute the posterior distribution of a treatment effect without relying on the computationally demanding posterior approximation. Second, we can guarantee that the posterior distribution concentrates around the true one as the sample size goes to infinity. Third, we can incorporate prior knowledge about a treatment effect into the prior distribution, improving the estimation efficiency. Our experimental results show that even in the small sample size setting, our method can accurately estimate the heterogeneous treatment effects and effectively quantify its estimation uncertainty.},
    number={18},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Horii, Shunsuke and Chikahara, Yoichi},
    year={2024},
    month={Mar.},
    pages={20420-20429}
}

@article{kinderman1977,
    author = {Kinderman, A. J. and Monahan, J. F.},
    title = {Computer Generation of Random Variables Using the Ratio of Uniform Deviates},
    year = {1977},
    issue_date = {Sept. 1977},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {3},
    number = {3},
    issn = {0098-3500},
    url = {https://doi.org/10.1145/355744.355750},
    doi = {10.1145/355744.355750},
    journal = {ACM Trans. Math. Softw.},
    month = {sep},
    pages = {257–260},
    numpages = {4}
}

@misc{hahn2019,
    title={Atlantic Causal Inference Conference (ACIC) Data Analysis Challenge 2017}, 
    author={P. Richard Hahn and Vincent Dorie and Jared S. Murray},
    year={2019},
    eprint={1905.09515},
    archivePrefix={arXiv},
    primaryClass={stat.ME},
    url={https://arxiv.org/abs/1905.09515}, 
}

@online{acic2019,
  title = {Atlantic Causal Inference Conference 2019 Data Challenge},
  author = {Gruber, Susan and Lefebvre, Geneviève and Schuster, Tibor and Piché, Alexandre},
  url = {https://sites.google.com/view/ACIC2019DataChallenge},
  urldate = {2024-09-02},
  year = {2019},
  organization = {Atlantic Causal Inference Conference}
}

@inproceedings{arora2019,
    author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {On Exact Computation with an Infinitely Wide Neural Net},
    url = {https://proceedings.neurips.cc/paper/2019/hash/dbc4d84bfcfe2284ba11beffb853a8c4-Abstract.html},
    volume = {32},
    year = {2019}
}

@software{petrillo2024b,
  author       = {Giacomo Petrillo},
  title        = {{Gattocrucco/bartz: The real treasure was the 
                   Markov chain samples we made along the way}},
  month        = oct,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.13931478},
  url          = {https://doi.org/10.5281/zenodo.13931478}
}

@software{petrillo2024c,
  author       = {Giacomo Petrillo},
  title        = {{Gattocrucco/lsqfitgp: Release early, release 
                   often, then hide in a foreign country and wait one
                   year}},
  month        = oct,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.21},
  doi          = {10.5281/zenodo.13930793},
  url          = {https://doi.org/10.5281/zenodo.13930793}
}

@misc{terenin2022,
    title={Gaussian Processes and Statistical Decision-making in Non-Euclidean Spaces}, 
    author={Alexander Terenin},
    year={2022},
    eprint={2202.10613},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/2202.10613}, 
}

@article{oreilly2022,
author = {O'Reilly, Eliza and Tran, Ngoc Mai},
title = {Stochastic Geometry to Generalize the Mondrian Process},
journal = {SIAM Journal on Mathematics of Data Science},
volume = {4},
number = {2},
pages = {531-552},
year = {2022},
doi = {10.1137/20M1354490},

URL = { 
    
        https://doi.org/10.1137/20M1354490
    
    

},
eprint = { 
    
        https://doi.org/10.1137/20M1354490
    
    

}
,
    abstract = { The stable under iteration (STIT) tessellation process is a stochastic process that produces a recursive partition of space with cut directions drawn independently from a distribution over the sphere. The case of random axis-aligned cuts is known as the Mondrian process. Random forests and Laplace kernel approximations built from the Mondrian process have led to efficient online learning methods and Bayesian optimization. In this work, we utilize tools from stochastic geometry to resolve some fundamental questions concerning STIT processes in machine learning. First, we show that STIT processes can be efficiently simulated by lifting to a higher-dimensional axis-aligned Mondrian process. Second, we characterize all possible kernels that STIT processes and their mixtures can approximate. We also give a uniform convergence rate for the approximation error of the STIT kernels to the targeted kernels, completely generalizing the work of Balog et al. [The Mondrian kernel, 2016] from the Mondrian case. Third, we obtain consistency results for STIT forests in density estimation and regression. Finally, we give a precise formula for the density estimator arising from a STIT forest. This allows for precise comparisons between the STIT forest, the STIT kernel, and the targeted kernel in density estimation. Our paper calls for further developments at the novel intersection of stochastic geometry and machine learning. }
}

@misc{epperly2024,
      title={Embrace rejection: Kernel matrix approximation by accelerated randomly pivoted Cholesky}, 
      author={Ethan N. Epperly and Joel A. Tropp and Robert J. Webber},
      year={2024},
      eprint={2410.03969},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2410.03969}, 
}

@inproceedings{chipman2006,
    author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {265--272},
 publisher = {MIT Press},
 title = {Bayesian Ensemble Learning},
 url = {https://proceedings.neurips.cc/paper/2006/hash/1706f191d760c78dfcec5012e43b6714-Abstract.html},
 volume = {19},
 year = {2006}
}

@book{daniels2023,
  author    = {Daniels, Michael J. and Linero, Antonio R. and Roy, Jason},
  title     = {Bayesian Nonparametrics for Causal Inference and Missing Data},
  year      = {2023},
  edition   = {1},
  publisher = {Chapman and Hall/CRC},
  doi       = {10.1201/9780429324222}
}

@software{petrillo2024d,
  author       = {Giacomo Petrillo},
  title        = {Gattocrucco/bart-gp-article: First version},
  month        = oct,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {first-version},
  doi          = {10.5281/zenodo.13997071},
  url          = {https://doi.org/10.5281/zenodo.13997071}
}
